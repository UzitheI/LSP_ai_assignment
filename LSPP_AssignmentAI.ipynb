{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BKCi4UnHVnpw"
   },
   "source": [
    "# Corporate Intelligence Extractor\n",
    "\n",
    "An advanced AI-powered system for extracting comprehensive company information from text data, featuring intelligent analysis and business intelligence capabilities.\n",
    "\n",
    "### **Unique Features:**\n",
    "- **Confidence Scoring**: Advanced confidence metrics for extraction quality assessment\n",
    "- **Smart Date Processing**: Fuzzy matching and intelligent date normalization\n",
    "- **Geographic Intelligence**: Location extraction with validation\n",
    "- **Industry Analytics**: Comprehensive sector analysis and insights\n",
    "- **Enhanced Data Export**: Multi-format output with enriched metadata\n",
    "- **Validation Framework**: Built-in quality assurance and data verification\n",
    "\n",
    "### **Advanced Output:**\n",
    "- Company information with confidence scores\n",
    "- Industry distribution analysis\n",
    "- Founder demographics and insights\n",
    "- Geographic mapping and validation\n",
    "- Decade-based founding trends\n",
    "- Enhanced CSV export with metadata"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "t9OWdeQdXRkF"
   },
   "source": [
    "## Setup and Installation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "66X_-rB9VoCa",
    "outputId": "829e494e-42cb-4378-f971-684a2211f498"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting langchain\n",
      "  Downloading langchain-0.3.27-py3-none-any.whl.metadata (7.8 kB)\n",
      "Collecting langchain-google-genai\n",
      "  Downloading langchain_google_genai-2.1.9-py3-none-any.whl.metadata (7.2 kB)\n",
      "Collecting pydantic\n",
      "  Using cached pydantic-2.11.7-py3-none-any.whl.metadata (67 kB)\n",
      "Collecting pandas\n",
      "  Using cached pandas-2.3.1-cp313-cp313-macosx_11_0_arm64.whl.metadata (91 kB)\n",
      "Collecting langchain-core<1.0.0,>=0.3.72 (from langchain)\n",
      "  Downloading langchain_core-0.3.74-py3-none-any.whl.metadata (5.8 kB)\n",
      "Collecting langchain-text-splitters<1.0.0,>=0.3.9 (from langchain)\n",
      "  Downloading langchain_text_splitters-0.3.9-py3-none-any.whl.metadata (1.9 kB)\n",
      "Collecting langsmith>=0.1.17 (from langchain)\n",
      "  Downloading langsmith-0.4.13-py3-none-any.whl.metadata (14 kB)\n",
      "Collecting SQLAlchemy<3,>=1.4 (from langchain)\n",
      "  Downloading sqlalchemy-2.0.43-cp313-cp313-macosx_11_0_arm64.whl.metadata (9.6 kB)\n",
      "Requirement already satisfied: requests<3,>=2 in ./.venv/lib/python3.13/site-packages (from langchain) (2.32.4)\n",
      "Requirement already satisfied: PyYAML>=5.3 in ./.venv/lib/python3.13/site-packages (from langchain) (6.0.2)\n",
      "Collecting annotated-types>=0.6.0 (from pydantic)\n",
      "  Using cached annotated_types-0.7.0-py3-none-any.whl.metadata (15 kB)\n",
      "Collecting pydantic-core==2.33.2 (from pydantic)\n",
      "  Using cached pydantic_core-2.33.2-cp313-cp313-macosx_11_0_arm64.whl.metadata (6.8 kB)\n",
      "Requirement already satisfied: typing-extensions>=4.12.2 in ./.venv/lib/python3.13/site-packages (from pydantic) (4.14.1)\n",
      "Collecting typing-inspection>=0.4.0 (from pydantic)\n",
      "  Using cached typing_inspection-0.4.1-py3-none-any.whl.metadata (2.6 kB)\n",
      "Collecting tenacity!=8.4.0,<10.0.0,>=8.1.0 (from langchain-core<1.0.0,>=0.3.72->langchain)\n",
      "  Using cached tenacity-9.1.2-py3-none-any.whl.metadata (1.2 kB)\n",
      "Collecting jsonpatch<2.0,>=1.33 (from langchain-core<1.0.0,>=0.3.72->langchain)\n",
      "  Using cached jsonpatch-1.33-py2.py3-none-any.whl.metadata (3.0 kB)\n",
      "Requirement already satisfied: packaging>=23.2 in ./.venv/lib/python3.13/site-packages (from langchain-core<1.0.0,>=0.3.72->langchain) (25.0)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in ./.venv/lib/python3.13/site-packages (from jsonpatch<2.0,>=1.33->langchain-core<1.0.0,>=0.3.72->langchain) (3.0.0)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in ./.venv/lib/python3.13/site-packages (from requests<3,>=2->langchain) (3.4.3)\n",
      "Requirement already satisfied: idna<4,>=2.5 in ./.venv/lib/python3.13/site-packages (from requests<3,>=2->langchain) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in ./.venv/lib/python3.13/site-packages (from requests<3,>=2->langchain) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in ./.venv/lib/python3.13/site-packages (from requests<3,>=2->langchain) (2025.8.3)\n",
      "Collecting filetype<2.0.0,>=1.2.0 (from langchain-google-genai)\n",
      "  Downloading filetype-1.2.0-py2.py3-none-any.whl.metadata (6.5 kB)\n",
      "Collecting google-ai-generativelanguage<0.7.0,>=0.6.18 (from langchain-google-genai)\n",
      "  Downloading google_ai_generativelanguage-0.6.18-py3-none-any.whl.metadata (9.8 kB)\n",
      "Collecting google-api-core!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.1 (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.1->google-ai-generativelanguage<0.7.0,>=0.6.18->langchain-google-genai)\n",
      "  Downloading google_api_core-2.25.1-py3-none-any.whl.metadata (3.0 kB)\n",
      "Collecting google-auth!=2.24.0,!=2.25.0,<3.0.0,>=2.14.1 (from google-ai-generativelanguage<0.7.0,>=0.6.18->langchain-google-genai)\n",
      "  Downloading google_auth-2.40.3-py2.py3-none-any.whl.metadata (6.2 kB)\n",
      "Collecting proto-plus<2.0.0,>=1.22.3 (from google-ai-generativelanguage<0.7.0,>=0.6.18->langchain-google-genai)\n",
      "  Downloading proto_plus-1.26.1-py3-none-any.whl.metadata (2.2 kB)\n",
      "Collecting protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<7.0.0,>=3.20.2 (from google-ai-generativelanguage<0.7.0,>=0.6.18->langchain-google-genai)\n",
      "  Using cached protobuf-6.31.1-cp39-abi3-macosx_10_9_universal2.whl.metadata (593 bytes)\n",
      "Collecting googleapis-common-protos<2.0.0,>=1.56.2 (from google-api-core!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.1->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.1->google-ai-generativelanguage<0.7.0,>=0.6.18->langchain-google-genai)\n",
      "  Downloading googleapis_common_protos-1.70.0-py3-none-any.whl.metadata (9.3 kB)\n",
      "Collecting grpcio<2.0.0,>=1.33.2 (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.1->google-ai-generativelanguage<0.7.0,>=0.6.18->langchain-google-genai)\n",
      "  Downloading grpcio-1.74.0-cp313-cp313-macosx_11_0_universal2.whl.metadata (3.8 kB)\n",
      "Collecting grpcio-status<2.0.0,>=1.33.2 (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.1->google-ai-generativelanguage<0.7.0,>=0.6.18->langchain-google-genai)\n",
      "  Downloading grpcio_status-1.74.0-py3-none-any.whl.metadata (1.1 kB)\n",
      "Collecting cachetools<6.0,>=2.0.0 (from google-auth!=2.24.0,!=2.25.0,<3.0.0,>=2.14.1->google-ai-generativelanguage<0.7.0,>=0.6.18->langchain-google-genai)\n",
      "  Downloading cachetools-5.5.2-py3-none-any.whl.metadata (5.4 kB)\n",
      "Collecting pyasn1-modules>=0.2.1 (from google-auth!=2.24.0,!=2.25.0,<3.0.0,>=2.14.1->google-ai-generativelanguage<0.7.0,>=0.6.18->langchain-google-genai)\n",
      "  Downloading pyasn1_modules-0.4.2-py3-none-any.whl.metadata (3.5 kB)\n",
      "Collecting rsa<5,>=3.1.4 (from google-auth!=2.24.0,!=2.25.0,<3.0.0,>=2.14.1->google-ai-generativelanguage<0.7.0,>=0.6.18->langchain-google-genai)\n",
      "  Downloading rsa-4.9.1-py3-none-any.whl.metadata (5.6 kB)\n",
      "Collecting pyasn1>=0.1.3 (from rsa<5,>=3.1.4->google-auth!=2.24.0,!=2.25.0,<3.0.0,>=2.14.1->google-ai-generativelanguage<0.7.0,>=0.6.18->langchain-google-genai)\n",
      "  Downloading pyasn1-0.6.1-py3-none-any.whl.metadata (8.4 kB)\n",
      "Collecting numpy>=1.26.0 (from pandas)\n",
      "  Using cached numpy-2.3.2-cp313-cp313-macosx_14_0_arm64.whl.metadata (62 kB)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in ./.venv/lib/python3.13/site-packages (from pandas) (2.9.0.post0)\n",
      "Collecting pytz>=2020.1 (from pandas)\n",
      "  Using cached pytz-2025.2-py2.py3-none-any.whl.metadata (22 kB)\n",
      "Collecting tzdata>=2022.7 (from pandas)\n",
      "  Using cached tzdata-2025.2-py2.py3-none-any.whl.metadata (1.4 kB)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in ./.venv/lib/python3.13/site-packages (from langsmith>=0.1.17->langchain) (0.28.1)\n",
      "Collecting orjson>=3.9.14 (from langsmith>=0.1.17->langchain)\n",
      "  Downloading orjson-3.11.1-cp313-cp313-macosx_15_0_arm64.whl.metadata (42 kB)\n",
      "Collecting requests-toolbelt>=1.0.0 (from langsmith>=0.1.17->langchain)\n",
      "  Using cached requests_toolbelt-1.0.0-py2.py3-none-any.whl.metadata (14 kB)\n",
      "Collecting zstandard>=0.23.0 (from langsmith>=0.1.17->langchain)\n",
      "  Using cached zstandard-0.23.0-cp313-cp313-macosx_11_0_arm64.whl.metadata (3.0 kB)\n",
      "Requirement already satisfied: anyio in ./.venv/lib/python3.13/site-packages (from httpx<1,>=0.23.0->langsmith>=0.1.17->langchain) (4.10.0)\n",
      "Requirement already satisfied: httpcore==1.* in ./.venv/lib/python3.13/site-packages (from httpx<1,>=0.23.0->langsmith>=0.1.17->langchain) (1.0.9)\n",
      "Requirement already satisfied: h11>=0.16 in ./.venv/lib/python3.13/site-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith>=0.1.17->langchain) (0.16.0)\n",
      "Requirement already satisfied: six>=1.5 in ./.venv/lib/python3.13/site-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
      "Requirement already satisfied: sniffio>=1.1 in ./.venv/lib/python3.13/site-packages (from anyio->httpx<1,>=0.23.0->langsmith>=0.1.17->langchain) (1.3.1)\n",
      "Downloading langchain-0.3.27-py3-none-any.whl (1.0 MB)\n",
      "\u001b[2K   \u001b[38;2;114;156;31mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m9.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hUsing cached pydantic-2.11.7-py3-none-any.whl (444 kB)\n",
      "Using cached pydantic_core-2.33.2-cp313-cp313-macosx_11_0_arm64.whl (1.8 MB)\n",
      "Downloading langchain_core-0.3.74-py3-none-any.whl (443 kB)\n",
      "Using cached jsonpatch-1.33-py2.py3-none-any.whl (12 kB)\n",
      "Downloading langchain_text_splitters-0.3.9-py3-none-any.whl (33 kB)\n",
      "Downloading sqlalchemy-2.0.43-cp313-cp313-macosx_11_0_arm64.whl (2.1 MB)\n",
      "\u001b[2K   \u001b[38;2;114;156;31mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m2.1/2.1 MB\u001b[0m \u001b[31m20.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hUsing cached tenacity-9.1.2-py3-none-any.whl (28 kB)\n",
      "Downloading langchain_google_genai-2.1.9-py3-none-any.whl (49 kB)\n",
      "Downloading filetype-1.2.0-py2.py3-none-any.whl (19 kB)\n",
      "Downloading google_ai_generativelanguage-0.6.18-py3-none-any.whl (1.4 MB)\n",
      "\u001b[2K   \u001b[38;2;114;156;31mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m1.4/1.4 MB\u001b[0m \u001b[31m20.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading google_api_core-2.25.1-py3-none-any.whl (160 kB)\n",
      "Downloading google_auth-2.40.3-py2.py3-none-any.whl (216 kB)\n",
      "Downloading cachetools-5.5.2-py3-none-any.whl (10 kB)\n",
      "Downloading googleapis_common_protos-1.70.0-py3-none-any.whl (294 kB)\n",
      "Downloading grpcio-1.74.0-cp313-cp313-macosx_11_0_universal2.whl (11.0 MB)\n",
      "\u001b[2K   \u001b[38;2;114;156;31mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m11.0/11.0 MB\u001b[0m \u001b[31m19.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m[36m0:00:01\u001b[0m36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading grpcio_status-1.74.0-py3-none-any.whl (14 kB)\n",
      "Downloading proto_plus-1.26.1-py3-none-any.whl (50 kB)\n",
      "Using cached protobuf-6.31.1-cp39-abi3-macosx_10_9_universal2.whl (425 kB)\n",
      "Downloading rsa-4.9.1-py3-none-any.whl (34 kB)\n",
      "Using cached pandas-2.3.1-cp313-cp313-macosx_11_0_arm64.whl (10.7 MB)\n",
      "Using cached annotated_types-0.7.0-py3-none-any.whl (13 kB)\n",
      "Downloading langsmith-0.4.13-py3-none-any.whl (372 kB)\n",
      "Using cached numpy-2.3.2-cp313-cp313-macosx_14_0_arm64.whl (5.1 MB)\n",
      "Downloading orjson-3.11.1-cp313-cp313-macosx_15_0_arm64.whl (129 kB)\n",
      "Downloading pyasn1-0.6.1-py3-none-any.whl (83 kB)\n",
      "Downloading pyasn1_modules-0.4.2-py3-none-any.whl (181 kB)\n",
      "Using cached pytz-2025.2-py2.py3-none-any.whl (509 kB)\n",
      "Using cached requests_toolbelt-1.0.0-py2.py3-none-any.whl (54 kB)\n",
      "Using cached typing_inspection-0.4.1-py3-none-any.whl (14 kB)\n",
      "Using cached tzdata-2025.2-py2.py3-none-any.whl (347 kB)\n",
      "Using cached zstandard-0.23.0-cp313-cp313-macosx_11_0_arm64.whl (633 kB)\n",
      "Installing collected packages: pytz, filetype, zstandard, tzdata, typing-inspection, tenacity, SQLAlchemy, pydantic-core, pyasn1, protobuf, orjson, numpy, jsonpatch, grpcio, cachetools, annotated-types, rsa, requests-toolbelt, pydantic, pyasn1-modules, proto-plus, pandas, googleapis-common-protos, langsmith, grpcio-status, google-auth, langchain-core, google-api-core, langchain-text-splitters, langchain, google-ai-generativelanguage, langchain-google-genai\n",
      "\u001b[2K   \u001b[38;2;114;156;31mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m32/32\u001b[0m [langchain-google-genai]32m30/32\u001b[0m [google-ai-generativelanguage]\n",
      "\u001b[1A\u001b[2KSuccessfully installed SQLAlchemy-2.0.43 annotated-types-0.7.0 cachetools-5.5.2 filetype-1.2.0 google-ai-generativelanguage-0.6.18 google-api-core-2.25.1 google-auth-2.40.3 googleapis-common-protos-1.70.0 grpcio-1.74.0 grpcio-status-1.74.0 jsonpatch-1.33 langchain-0.3.27 langchain-core-0.3.74 langchain-google-genai-2.1.9 langchain-text-splitters-0.3.9 langsmith-0.4.13 numpy-2.3.2 orjson-3.11.1 pandas-2.3.1 proto-plus-1.26.1 protobuf-6.31.1 pyasn1-0.6.1 pyasn1-modules-0.4.2 pydantic-2.11.7 pydantic-core-2.33.2 pytz-2025.2 requests-toolbelt-1.0.0 rsa-4.9.1 tenacity-9.1.2 typing-inspection-0.4.1 tzdata-2025.2 zstandard-0.23.0\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.1.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# required packages\n",
    "!pip install langchain langchain-google-genai pydantic pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9XeFAOSTV8qP"
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'google.colab'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 8\u001b[39m\n\u001b[32m      6\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtyping\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m List, Dict, Optional, Any\n\u001b[32m      7\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpydantic\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m BaseModel, Field\n\u001b[32m----> \u001b[39m\u001b[32m8\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mgoogle\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mcolab\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m userdata\n\u001b[32m     10\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mlangchain_google_genai\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m ChatGoogleGenerativeAI\n\u001b[32m     11\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mlangchain_core\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mprompts\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m ChatPromptTemplate, PromptTemplate\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'google.colab'"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import re\n",
    "from datetime import datetime\n",
    "from typing import List, Dict, Optional, Any\n",
    "from pydantic import BaseModel, Field\n",
    "from google.colab import userdata\n",
    "\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "from langchain_core.prompts import ChatPromptTemplate, PromptTemplate\n",
    "from langchain_core.output_parsers import JsonOutputParser, PydanticOutputParser\n",
    "from langchain_core.runnables import RunnableLambda, RunnablePassthrough\n",
    "from langchain.schema import BaseOutputParser"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7aHWt2V_XXey"
   },
   "source": [
    "## API Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Ws0NI_hyWN5K"
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'userdata' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# Configure Gemini API\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m os.environ[\u001b[33m\"\u001b[39m\u001b[33mGOOGLE_API_KEY\u001b[39m\u001b[33m\"\u001b[39m] = \u001b[43muserdata\u001b[49m.get(\u001b[33m\"\u001b[39m\u001b[33mGEMINI_API_KEY\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[31mNameError\u001b[39m: name 'userdata' is not defined"
     ]
    }
   ],
   "source": [
    "#adding gemini api key (this is for google collab rn)\n",
    "os.environ[\"GOOGLE_API_KEY\"] = userdata.get(\"GEMINI_API_KEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6Ej485gZWUSH"
   },
   "outputs": [],
   "source": [
    "llm = ChatGoogleGenerativeAI(\n",
    "    model=\"gemini-2.5-flash-lite\",\n",
    "    temperature=0.3,\n",
    "    max_tokens=None\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "dJkntRMUWZFd",
    "outputId": "f0806d23-d82d-4172-a48b-8459860de01d"
   },
   "outputs": [],
   "source": [
    "print(\"Testing Enhanced Date Normalization:\")\n",
    "test_dates = [\"1946\", \"founded in 1955\", \"est. 2001\", \"started 1999\"]\n",
    "for date_str in test_dates:\n",
    "    normalized = normalize_date_advanced(date_str)\n",
    "    print(f\"  '{date_str}' -> {normalized}\")\n",
    "\n",
    "print(\"\\nTesting Location Extraction:\")\n",
    "test_locations = [\"Founded in New York\", \"California-based company\", \"Tokyo, Japan headquarters\"]\n",
    "for loc_str in test_locations:\n",
    "    location = extract_location(loc_str)\n",
    "    print(f\"  '{loc_str}' -> {location}\")\n",
    "\n",
    "print(\"\\nTesting Industry Classification:\")\n",
    "test_descriptions = [\n",
    "    \"technology company developing software\",\n",
    "    \"automotive manufacturer\",\n",
    "    \"financial services provider\"\n",
    "]\n",
    "for desc in test_descriptions:\n",
    "    industry = classify_industry(desc)\n",
    "    print(f\"  '{desc}' -> {industry}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hy4UdYlfXdpl"
   },
   "source": [
    "### Advanced Company Extraction Prompt Template\n",
    "\n",
    "This template provides comprehensive extraction guidelines for the AI model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qzVxwxOWWf--"
   },
   "outputs": [],
   "source": [
    "class CompanyInfo(BaseModel):\n",
    "    \"\"\"Enhanced data model for extracted company information with advanced features.\"\"\"\n",
    "    company_name: str = Field(description=\"The full name of the company\")\n",
    "    founding_date: str = Field(description=\"The founding date in YYYY-MM-DD format\")\n",
    "    founders: List[str] = Field(description=\"List of founder names\")\n",
    "\n",
    "    confidence_score: float = Field(default=0.0, description=\"Extraction confidence (0-1)\")\n",
    "    company_age: Optional[int] = Field(default=None, description=\"Age of company in years\")\n",
    "    founder_count: int = Field(default=0, description=\"Number of founders\")\n",
    "    founding_location: Optional[str] = Field(default=None, description=\"City/Country of founding\")\n",
    "    industry_category: Optional[str] = Field(default=None, description=\"Inferred industry category\")\n",
    "\n",
    "    class Config:\n",
    "        json_schema_extra = {\n",
    "            \"example\": {\n",
    "                \"company_name\": \"Microsoft Corporation\",\n",
    "                \"founding_date\": \"1975-04-04\",\n",
    "                \"founders\": [\"Bill Gates\", \"Paul Allen\"],\n",
    "                \"confidence_score\": 0.95,\n",
    "                \"company_age\": 49,\n",
    "                \"founder_count\": 2,\n",
    "                \"founding_location\": \"Albuquerque, New Mexico\",\n",
    "                \"industry_category\": \"Technology\"\n",
    "            }\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jHs0987SW_JJ"
   },
   "outputs": [],
   "source": [
    "class CompanyExtractionResult(BaseModel):\n",
    "    \"\"\"Container for multiple company extractions from a paragraph.\"\"\"\n",
    "    companies: List[CompanyInfo] = Field(description=\"List of extracted companies\")\n",
    "    paragraph_processed: str = Field(description=\"The original paragraph text\")\n",
    "    processing_time: float = Field(default=0.0, description=\"Time taken to process\")\n",
    "    avg_confidence: float = Field(default=0.0, description=\"Average confidence score\")\n",
    "\n",
    "class ExtractionMetrics(BaseModel):\n",
    "    \"\"\"Quality and performance metrics for the extraction process.\"\"\"\n",
    "    total_companies: int = Field(description=\"Total companies extracted\")\n",
    "    avg_confidence_score: float = Field(description=\"Average confidence across all extractions\")\n",
    "    high_confidence_count: int = Field(description=\"Number of high-confidence extractions (>0.8)\")\n",
    "    processing_time_total: float = Field(description=\"Total processing time in seconds\")\n",
    "    paragraphs_processed: int = Field(description=\"Number of paragraphs processed\")\n",
    "    extraction_rate: float = Field(description=\"Companies per paragraph ratio\")\n",
    "\n",
    "class IndustryAnalytics(BaseModel):\n",
    "    \"\"\"Industry-specific analytics and insights.\"\"\"\n",
    "    industry_distribution: Dict[str, int] = Field(description=\"Count by industry category\")\n",
    "    decade_distribution: Dict[str, int] = Field(description=\"Companies founded per decade\")\n",
    "    founder_analytics: Dict[str, Any] = Field(description=\"Founder-related statistics\")\n",
    "    geographic_distribution: Dict[str, int] = Field(description=\"Geographic founding locations\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4KfCMgBiXlBI"
   },
   "source": [
    "## Testing Phase"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Xpgl2J76YGbr"
   },
   "source": [
    "### Step 1: Breaking Text into Paragraphs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ENRPUAlvXmky"
   },
   "outputs": [],
   "source": [
    "# Sample essay\n",
    "essay_text = \"\"\"\n",
    "In the ever-evolving landscape of global commerce, the origin stories of major corporations are not merely tales of personal ambition and entrepreneurial spirit but also reflections of broader socio-economic trends and technological revolutions that have reshaped industries. These narratives, which often begin with modest ambitions, unfold into chronicles of innovation and strategic foresight that define industries and set benchmarks for future enterprises.\n",
    "\n",
    "Early Foundations: Pioneers of Industry\n",
    "One of the earliest examples is The Coca-Cola Company, founded on May 8, 1886, by Dr. John Stith Pemberton in Atlanta, Georgia. Initially sold at Jacob's Pharmacy as a medicinal beverage, Coca-Cola would become one of the most recognized brands worldwide, revolutionizing the beverage industry.\n",
    "Similarly, Sony Corporation was established on May 7, 1946, by Masaru Ibuka and Akio Morita in Tokyo, Japan. Starting with repairing and building electrical equipment in post-war Japan, Sony would grow to pioneer electronics, entertainment, and technology.\n",
    "As the mid-20th century progressed, McDonald's Corporation emerged as a game-changer in the fast-food industry. Founded on April 15, 1955, in Des Plaines, Illinois, by Ray Kroc, McDonald's built upon the original concept of Richard and Maurice McDonald to standardize and scale fast-food service globally. Around the same period, Intel Corporation was established on July 18, 1968, by Robert Noyce and Gordon Moore in Mountain View, California\n",
    "\n",
    "driving advancements in semiconductors and microprocessors that became the backbone of modern computing.\n",
    "\n",
    "The Rise of Technology Titans\n",
    "Samsung Electronics Co., Ltd., founded on January 13, 1969, by Lee Byung-chul in Su-dong, South Korea, initially focused on producing electrical appliances like televisions and refrigerators. As Samsung expanded into semiconductors, telecommunications, and digital media, it\n",
    "grew into a global technology leader. Similarly, Microsoft Corporation was founded on April 4, 1975, by Bill Gates and Paul Allen in Albuquerque, New Mexico, with the vision of placing a computer on every desk and in every home.\n",
    "In Cupertino, California, Apple Inc. was born on April 1, 1976, founded by Steve Jobs, Steve Wozniak, and Ronald Wayne. Their mission to make personal computing accessible and elegant revolutionized technology and design. A few years later, Oracle Corporation was established on June 16, 1977, by Larry Ellison, Bob Miner, and Ed Oates in Santa Clara, California.\n",
    "Specializing in relational databases, Oracle would become a cornerstone of enterprise software and cloud computing.\n",
    "NVIDIA Corporation, founded on April 5, 1993, by Jensen Huang, Chris Malachowsky, and Curtis Priem in Santa Clara, California, began with a focus on graphics processing units (GPUs) for gaming. Today, NVIDIA is a leader in artificial intelligence, deep learning, and autonomous systems, showcasing the power of continuous innovation.\n",
    "\n",
    "E-Commerce and the Internet Revolution\n",
    "The 1990s witnessed a dramatic shift toward e-commerce and internet technologies. Amazon.com Inc. was founded on July 5, 1994, by Jeff Bezos in a garage in Bellevue, Washington, with the vision of becoming the world's largest online bookstore. This vision rapidly expanded to encompass\n",
    "e-commerce, cloud computing, and digital streaming. Similarly, Google LLC was founded on September 4, 1998, by Larry Page and Sergey Brin, PhD students at Stanford University, in a garage in Menlo Park, California.\n",
    "Google's mission to \"organize the world's information\" transformed how we search, learn, and connect.\n",
    "In Asia, Alibaba Group Holding Limited was founded on June 28, 1999, by Jack Ma and 18 colleagues in Hangzhou, China. Originally an e-commerce platform connecting manufacturers with buyers, Alibaba expanded into cloud\n",
    "\n",
    "computing, digital entertainment, and financial technology, becoming a global powerhouse.\n",
    "In Europe, SAP SE was founded on April 1, 1972, by Dietmar Hopp,\n",
    "Hans-Werner Hector, Hasso Plattner, Klaus Tschira, and Claus Wellenreuther in Weinheim, Germany. Specializing in enterprise resource planning (ERP) software, SAP revolutionized how businesses manage operations and data.\n",
    "\n",
    "Social Media and Digital Platforms\n",
    "The 2000s brought a wave of social media and digital platforms that reshaped communication and commerce. LinkedIn Corporation was founded on December 28, 2002, by Reid Hoffman and a team from PayPal and Socialnet.com in Mountain View, California, focusing on professional networking.\n",
    "Facebook, Inc. (now Meta Platforms, Inc.) was launched on February 4, 2004, by Mark Zuckerberg and his college roommates in Cambridge, Massachusetts, evolving into a global social networking behemoth.\n",
    "Another transformative platform, Twitter, Inc., was founded on March 21, 2006, by Jack Dorsey, Biz Stone, and Evan Williams in San Francisco, California. Starting as a microblogging service, Twitter became a critical tool for communication and social commentary. Spotify AB, founded on April 23, 2006, by Daniel Ek and Martin Lorentzon in Stockholm, Sweden, leveraged streaming technology to democratize music consumption, fundamentally altering the music industry.\n",
    "In the realm of video-sharing, YouTube LLC was founded on February 14, 2005, by Steve Chen, Chad Hurley, and Jawed Karim in San Mateo, California. YouTube became the leading platform for user-generated video content, influencing global culture and media consumption.\n",
    "\n",
    "Innovators in Modern Technology\n",
    "Tesla, Inc., founded on July 1, 2003, by a group including Elon Musk, Martin Eberhard, Marc Tarpenning, JB Straubel, and Ian Wright, in San Carlos, California, championed the transition to sustainable energy with its electric vehicles and energy solutions. Airbnb, Inc., founded in August 2008 by Brian Chesky, Joe Gebbia, and Nathan Blecharczyk in San Francisco, California, disrupted traditional hospitality with its peer-to-peer lodging platform.\n",
    "In the realm of fintech, PayPal Holdings, Inc. was established in December 1998 by Peter Thiel, Max Levchin, Luke Nosek, and Ken Howery in Palo Alto,\n",
    "\n",
    "California. Originally a cryptography company, PayPal became a global leader in online payments. Stripe, Inc., founded in 2010 by Patrick and John Collison in Palo Alto, California, followed suit, simplifying online payments and enabling digital commerce.\n",
    "Square, Inc. (now Block, Inc.), founded on February 20, 2009, by Jack Dorsey and Jim McKelvey in San Francisco, California, revolutionized mobile payment systems with its simple and accessible card readers.\n",
    "\n",
    "Recent Disruptors\n",
    "Zoom Video Communications, Inc. was founded on April 21, 2011, by Eric Yuan in San Jose, California. Initially designed for video conferencing, Zoom became essential during the COVID-19 pandemic, transforming remote work and communication. Slack Technologies, LLC, founded in 2009 by Stewart Butterfield, Eric Costello, Cal Henderson, and Serguei Mourachov in Vancouver, Canada, redefined workplace communication with its innovative messaging platform.\n",
    "Rivian Automotive, Inc., founded on June 23, 2009, by RJ Scaringe in Plymouth, Michigan, entered the electric vehicle market with a focus on adventure and sustainability. SpaceX, established on March 14, 2002, by Elon Musk in Hawthorne, California, revolutionized aerospace with reusable rockets and ambitious plans for Mars exploration.\n",
    "TikTok, developed by ByteDance and launched in September 2016 by Zhang Yiming in Beijing, China, revolutionized short-form video content, becoming a cultural phenomenon worldwide.\n",
    "\n",
    "Conclusion\n",
    "These corporations, with their diverse beginnings and visionary founders, exemplify the interplay of innovation, timing, and strategic foresight that shapes industries and transforms markets. From repairing electronics in post-war Japan to building global e-commerce empires and redefining space exploration, their stories are milestones in the narrative of global economic transformation. Each reflects not only the aspirations of their founders but also the technological advancements and socio-economic trends of their time, serving as inspirations for future innovators.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "uh-hoT2IXsp-"
   },
   "outputs": [],
   "source": [
    "def split_into_paragraphs(text: str) -> List[str]:\n",
    "    \"\"\"Split text into paragraphs and remove empty ones.\"\"\"\n",
    "    paragraphs = [p.strip() for p in text.split('\\n\\n') if p.strip()]\n",
    "    return [p for p in paragraphs if len(p) > 50]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "uBlmrpLiXzLt",
    "outputId": "56300dbf-3c7a-4c4a-9b03-f1df6e4d6eed"
   },
   "outputs": [],
   "source": [
    "# paragraph splitting\n",
    "paragraphs = split_into_paragraphs(essay_text)\n",
    "print(f\"Total paragraphs found: {len(paragraphs)}\")\n",
    "print(\"First paragraph preview:\")\n",
    "print(paragraphs[0][:200] + \"...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "N9k1PWtPYCBT"
   },
   "source": [
    "### Step 2: Fixing Date Formats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "uHTvmQxxYDKO"
   },
   "outputs": [],
   "source": [
    "def normalize_date_advanced(date_str: str) -> tuple[str, float]:\n",
    "    \"\"\"\n",
    "    Advanced date normalization with confidence scoring.\n",
    "    Returns: (normalized_date, confidence_score)\n",
    "    \"\"\"\n",
    "    try:\n",
    "        date_str = date_str.strip().lower()\n",
    "        date_str = re.sub(r'^(on|in)\\s+', '', date_str)\n",
    "\n",
    "        confidence = 1.0 #starting with high confidence becase we're expecing best case scenario\n",
    "\n",
    "        month_patterns = {\n",
    "            'january': '01', 'jan': '01', 'february': '02', 'feb': '02',\n",
    "            'march': '03', 'mar': '03', 'april': '04', 'apr': '04',\n",
    "            'may': '05', 'june': '06', 'jun': '06', 'july': '07', 'jul': '07',\n",
    "            'august': '08', 'aug': '08', 'september': '09', 'sep': '09', 'sept': '09',\n",
    "            'october': '10', 'oct': '10', 'november': '11', 'nov': '11',\n",
    "            'december': '12', 'dec': '12'\n",
    "        }\n",
    "\n",
    "        year_pattern = r'\\b(19|20)\\d{2}\\b'\n",
    "        year_match = re.search(year_pattern, date_str)\n",
    "        if not year_match:\n",
    "            return \"1900-01-01\", 0.1\n",
    "\n",
    "        year = year_match.group()\n",
    "\n",
    "        full_date_patterns = [\n",
    "            (r'\\b(\\w+)\\s+(\\d{1,2}),?\\s+(\\d{4})\\b', 'month_day_year'), \n",
    "            (r'\\b(\\d{1,2})[/-](\\d{1,2})[/-](\\d{4})\\b', 'mm_dd_yyyy'),\n",
    "            (r'\\b(\\d{4})[/-](\\d{1,2})[/-](\\d{1,2})\\b', 'yyyy_mm_dd'),\n",
    "        ]\n",
    "\n",
    "        for pattern, format_type in full_date_patterns:\n",
    "            match = re.search(pattern, date_str)\n",
    "            if match:\n",
    "                parts = match.groups()\n",
    "                try:\n",
    "                    if format_type == 'month_day_year':\n",
    "                        month_name, day, year = parts\n",
    "                        if month_name in month_patterns:\n",
    "                            month = month_patterns[month_name]\n",
    "                            return f\"{year}-{month}-{day.zfill(2)}\", confidence\n",
    "                    elif format_type == 'mm_dd_yyyy':\n",
    "                        month, day, year = parts\n",
    "                        return f\"{year}-{month.zfill(2)}-{day.zfill(2)}\", confidence\n",
    "                    elif format_type == 'yyyy_mm_dd':\n",
    "                        year, month, day = parts\n",
    "                        return f\"{year}-{month.zfill(2)}-{day.zfill(2)}\", confidence\n",
    "                except:\n",
    "                    continue\n",
    "\n",
    "        for month_name, month_num in month_patterns.items():\n",
    "            if month_name in date_str:\n",
    "                confidence = 0.7 \n",
    "                return f\"{year}-{month_num}-01\", confidence\n",
    "\n",
    "        return f\"{year}-01-01\", 0.5\n",
    "\n",
    "    except Exception as e:\n",
    "        return \"1900-01-01\", 0.1\n",
    "\n",
    "def extract_location(text: str) -> tuple[Optional[str], float]:\n",
    "    \"\"\"Extract founding location with confidence scoring.\"\"\"\n",
    "    location_patterns = [\n",
    "        r'in ([A-Z][a-z]+(?:\\s+[A-Z][a-z]+)*),?\\s+([A-Z][a-z]+(?:\\s+[A-Z][a-z]+)*)', #city, state\n",
    "        r'in ([A-Z][a-z]+(?:\\s+[A-Z][a-z]+)*)', #location\n",
    "    ]\n",
    "\n",
    "    for pattern in location_patterns:\n",
    "        match = re.search(pattern, text)\n",
    "        if match:\n",
    "            if len(match.groups()) == 2:\n",
    "                city, region = match.groups()\n",
    "                return f\"{city}, {region}\", 0.9\n",
    "            else:\n",
    "                return match.group(1), 0.7\n",
    "\n",
    "    return None, 0.0\n",
    "\n",
    "def infer_industry(company_name: str, context: str) -> tuple[Optional[str], float]:\n",
    "    \"\"\"Infer industry category from company name and context.\"\"\"\n",
    "    industry_keywords = {\n",
    "        'Technology': ['software', 'tech', 'computer', 'digital', 'electronics', 'semiconductor', 'AI', 'data'],\n",
    "        'E-commerce': ['online', 'e-commerce', 'marketplace', 'shopping', 'retail'],\n",
    "        'Social Media': ['social', 'networking', 'media', 'platform', 'communication'],\n",
    "        'Automotive': ['automotive', 'car', 'vehicle', 'electric', 'transportation'],\n",
    "        'Finance': ['financial', 'payment', 'fintech', 'banking', 'investment'],\n",
    "        'Entertainment': ['entertainment', 'streaming', 'music', 'video', 'gaming'],\n",
    "        'Food & Beverage': ['food', 'beverage', 'restaurant', 'fast-food'],\n",
    "        'Aerospace': ['aerospace', 'space', 'aviation', 'rocket'],\n",
    "        'Healthcare': ['health', 'medical', 'pharmaceutical', 'biotech']\n",
    "    }\n",
    "\n",
    "    combined_text = f\"{company_name} {context}\".lower()\n",
    "\n",
    "    for industry, keywords in industry_keywords.items():\n",
    "        matches = sum(1 for keyword in keywords if keyword in combined_text)\n",
    "        if matches > 0:\n",
    "            confidence = min(0.9, matches * 0.3)\n",
    "            return industry, confidence\n",
    "\n",
    "    return None, 0.0\n",
    "\n",
    "def calculate_company_age(founding_date: str) -> Optional[int]:\n",
    "    \"\"\"Calculate company age in years.\"\"\"\n",
    "    try:\n",
    "        founding_year = int(founding_date.split('-')[0])\n",
    "        current_year = datetime.now().year\n",
    "        return current_year - founding_year\n",
    "    except:\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "AOK0GQhxYQyv",
    "outputId": "78215f87-2bfd-4603-e447-be9844311dce"
   },
   "outputs": [],
   "source": [
    "test_dates = [\n",
    "    \"May 8, 1886\",\n",
    "    \"April 4, 1975\",\n",
    "    \"2009\",\n",
    "    \"August 2008\",\n",
    "    \"December 1998\"\n",
    "]\n",
    "\n",
    "print(\"ðŸ” Testing Enhanced Date Normalization:\")\n",
    "print(\"-\" * 50)\n",
    "for date in test_dates:\n",
    "    normalized, confidence = normalize_date_advanced(date)\n",
    "    print(f\"'{date}' â†’ '{normalized}' (confidence: {confidence:.2f})\")\n",
    "\n",
    "print(\"\\nðŸŒ Testing Location Extraction:\")\n",
    "print(\"-\" * 40)\n",
    "test_locations = [\n",
    "    \"founded in San Francisco, California\",\n",
    "    \"established in Tokyo, Japan\",\n",
    "    \"created in London\"\n",
    "]\n",
    "\n",
    "for text in test_locations:\n",
    "    location, confidence = extract_location(text)\n",
    "    print(f\"'{text}' â†’ Location: {location} (confidence: {confidence:.2f})\")\n",
    "\n",
    "print(\"\\nðŸ­ Testing Industry Classification:\")\n",
    "print(\"-\" * 45)\n",
    "test_companies = [\n",
    "    (\"Microsoft Corporation\", \"computer software technology\"),\n",
    "    (\"Tesla Inc\", \"electric vehicle automotive\"),\n",
    "    (\"Facebook\", \"social networking platform\")\n",
    "]\n",
    "\n",
    "for company, context in test_companies:\n",
    "    industry, confidence = infer_industry(company, context)\n",
    "    print(f\"'{company}' â†’ Industry: {industry} (confidence: {confidence:.2f})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_lSS5893YYFX"
   },
   "source": [
    "### Step 3: Testing with One Paragraph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VyN9qhi_YcHG"
   },
   "outputs": [],
   "source": [
    "# Enhanced extraction prompt with confidence scoring\n",
    "extraction_prompt = ChatPromptTemplate.from_template(\"\"\"\n",
    "You are an elite corporate intelligence analyst with expertise in extracting structured business information from unstructured text. Your mission is to perform comprehensive company data extraction with maximum accuracy.\n",
    "\n",
    "**EXTRACTION TARGETS:**\n",
    "For each company mentioned, extract:\n",
    "1. **Company Name**: Full official legal name (prefer complete form over abbreviations)\n",
    "2. **Founding Date**: Exact date as mentioned (maintain original format precision)\n",
    "3. **Founders**: All individual founders mentioned (separate persons, not groups)\n",
    "4. **Confidence Score**: Your confidence in the extraction accuracy (0.0 to 1.0)\n",
    "5. **Location**: Founding city/country if mentioned\n",
    "6. **Industry Context**: Brief industry description based on context\n",
    "\n",
    "**EXTRACTION PROTOCOLS:**\n",
    "- Extract ALL companies in the paragraph (comprehensive scan)\n",
    "- Prioritize precision over speed - verify each extraction\n",
    "- Use exact company names as written in source text\n",
    "- Include all founders mentioned individually\n",
    "- Assign confidence scores based on information clarity\n",
    "- If founding location is mentioned, extract it precisely\n",
    "\n",
    "Return results in this enhanced JSON structure:\n",
    "{{\n",
    "  \"companies\": [\n",
    "    {{\n",
    "      \"company_name\": \"Complete Official Company Name\",\n",
    "      \"founding_date\": \"exact date as mentioned in text\",\n",
    "      \"founders\": [\"Individual Founder 1\", \"Individual Founder 2\"],\n",
    "      \"confidence_score\": 0.95,\n",
    "      \"founding_location\": \"City, State/Country (if mentioned)\",\n",
    "      \"industry_context\": \"Brief industry description\"\n",
    "    }}\n",
    "  ]\n",
    "}}\n",
    "\n",
    "**TEXT TO ANALYZE:**\n",
    "{paragraph}\n",
    "\n",
    "**AI RESPONSE:**\n",
    "\"\"\")\n",
    "\n",
    "#enhanced extraction chain\n",
    "extraction_chain = extraction_prompt | llm | JsonOutputParser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "2a5BQi7tYjgd",
    "outputId": "cd6bb011-4d84-49bf-f7db-68b7922f3e7a"
   },
   "outputs": [],
   "source": [
    "# testing the extraction on a single paragraph\n",
    "test_paragraph = paragraphs[1]\n",
    "print(\"Testing extraction on paragraph:\")\n",
    "print(test_paragraph[:200] + \"...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Fvqnu8DDYr6b",
    "outputId": "5f96088a-5d06-46d2-dfd0-be27da8fc36a"
   },
   "outputs": [],
   "source": [
    "#extracting\n",
    "try:\n",
    "    result = extraction_chain.invoke({\"paragraph\": test_paragraph})\n",
    "    print(\"\\nExtraction Result:\")\n",
    "    print(result)\n",
    "except Exception as e:\n",
    "    print(f\" Extraction failed: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "b0sAghq2Yv1U"
   },
   "outputs": [],
   "source": [
    "def process_extraction_result_advanced(raw_result: Dict, original_paragraph: str) -> List[CompanyInfo]:\n",
    "    \"\"\"Advanced processing with enhanced features and validation.\"\"\"\n",
    "    companies = []\n",
    "\n",
    "    if \"companies\" in raw_result:\n",
    "        for company_data in raw_result[\"companies\"]:\n",
    "            try:\n",
    "                raw_date = company_data.get(\"founding_date\", \"\")\n",
    "                if raw_date:\n",
    "                    normalized_date, date_confidence = normalize_date_advanced(raw_date)\n",
    "                else:\n",
    "                    normalized_date, date_confidence = \"1900-01-01\", 0.1\n",
    "\n",
    "                founders = company_data.get(\"founders\", [])\n",
    "                if isinstance(founders, str):\n",
    "                    founders = [founders]\n",
    "\n",
    "                # standardized founder names\n",
    "                cleaned_founders = []\n",
    "                for founder in founders:\n",
    "                    if founder and founder.strip():\n",
    "                        clean_name = founder.strip()\n",
    "                        clean_name = re.sub(r'^(Dr\\.|Prof\\.|Mr\\.|Ms\\.|Mrs\\.)\\s+', '', clean_name)\n",
    "                        clean_name = re.sub(r'\\s+', ' ', clean_name) \n",
    "                        if clean_name and len(clean_name) > 1:\n",
    "                            cleaned_founders.append(clean_name)\n",
    "\n",
    "                # Extract additional information\n",
    "                company_name = company_data.get(\"company_name\", \"Unknown\")\n",
    "                extraction_confidence = float(company_data.get(\"confidence_score\", 0.5))\n",
    "\n",
    "                company_age = calculate_company_age(normalized_date)\n",
    "\n",
    "                founding_location = company_data.get(\"founding_location\")\n",
    "                if not founding_location:\n",
    "                    founding_location, _ = extract_location(original_paragraph)\n",
    "\n",
    "                # Infer industry\n",
    "                industry_context = company_data.get(\"industry_context\", \"\")\n",
    "                if not industry_context:\n",
    "                    industry_category, _ = infer_industry(company_name, original_paragraph)\n",
    "                else:\n",
    "                    industry_category, _ = infer_industry(company_name, industry_context)\n",
    "\n",
    "                final_confidence = extraction_confidence * date_confidence\n",
    "                if len(cleaned_founders) > 0:\n",
    "                    final_confidence *= 1.1  # Boost for having founders\n",
    "                if founding_location:\n",
    "                    final_confidence *= 1.1  # Boost for location info\n",
    "                final_confidence = min(1.0, final_confidence)\n",
    "\n",
    "                company_info = CompanyInfo(\n",
    "                    company_name=company_name,\n",
    "                    founding_date=normalized_date,\n",
    "                    founders=cleaned_founders,\n",
    "                    confidence_score=round(final_confidence, 3),\n",
    "                    company_age=company_age,\n",
    "                    founder_count=len(cleaned_founders),\n",
    "                    founding_location=founding_location,\n",
    "                    industry_category=industry_category\n",
    "                )\n",
    "                companies.append(company_info)\n",
    "\n",
    "                if final_confidence > 0.8:\n",
    "                    print(f\"âœ… High-confidence extraction: {company_name} (confidence: {final_confidence:.3f})\")\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"âŒ Error processing company data: {e}\")\n",
    "                continue\n",
    "\n",
    "    return companies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "saDsuT0zY2CE",
    "outputId": "b24666a3-aafe-4f3f-84d8-8d0a41b21e42"
   },
   "outputs": [],
   "source": [
    "# Enhanced testing with advanced processing\n",
    "if 'result' in locals():\n",
    "    processed_companies = process_extraction_result_advanced(result, test_paragraph)\n",
    "    print(\"\\nðŸŽ¯ Enhanced Processing Results:\")\n",
    "    print(\"=\" * 60)\n",
    "    for company in processed_companies:\n",
    "        print(f\"\\nðŸ¢ Company: {company.company_name}\")\n",
    "        print(f\"ðŸ“… Founded: {company.founding_date} ({company.company_age} years old)\" if company.company_age else f\"ðŸ“… Founded: {company.founding_date}\")\n",
    "        print(f\"ðŸ‘¥ Founders: {', '.join(company.founders)} ({company.founder_count} total)\")\n",
    "        print(f\"ðŸ“ Location: {company.founding_location}\" if company.founding_location else \"ðŸ“ Location: Not specified\")\n",
    "        print(f\"ðŸ­ Industry: {company.industry_category}\" if company.industry_category else \"ðŸ­ Industry: Not classified\")\n",
    "        print(f\"â­ Confidence: {company.confidence_score:.1%}\")\n",
    "        print(\"-\" * 40)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OGg4FIM-Y_jC"
   },
   "source": [
    "## Building the Complete System"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kPaDb6gvZFXV"
   },
   "source": [
    "### Creating the Main Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Mrt6VmuUZGJ2"
   },
   "outputs": [],
   "source": [
    "class AdvancedCompanyExtractionPipeline:\n",
    "    \"\"\"ðŸš€ Next-generation corporate intelligence extraction system with advanced analytics.\"\"\"\n",
    "\n",
    "    def __init__(self, llm):\n",
    "        self.llm = llm\n",
    "        self.companies_extracted = []\n",
    "        self.processing_metrics = []\n",
    "\n",
    "        # Enhanced extraction prompt with multi-modal capabilities\n",
    "        self.extraction_prompt = ChatPromptTemplate.from_template(\"\"\"\n",
    "You are an elite corporate intelligence analyst specializing in extracting structured business data from narrative text. Your analysis must be thorough, accurate, and confidence-rated.\n",
    "\n",
    "ðŸŽ¯ **MISSION**: Extract comprehensive company intelligence from the provided text segment.\n",
    "\n",
    "ðŸ“Š **REQUIRED DATA POINTS** (for each company):\n",
    "1. **Company Name**: Official legal name (prefer full form over abbreviations)\n",
    "2. **Founding Date**: Precise date as mentioned in source\n",
    "3. **Founders**: Individual founder names (separate people, not collective terms)\n",
    "4. **Confidence Assessment**: Rate your extraction confidence (0.0-1.0)\n",
    "5. **Geographic Origin**: Founding location if specified\n",
    "6. **Industry Classification**: Business sector based on context clues\n",
    "\n",
    "ðŸ” **ANALYSIS PROTOCOLS**:\n",
    "- Scan entire paragraph for ALL company mentions\n",
    "- Verify founder names are individuals, not groups/teams\n",
    "- Maintain source accuracy - extract exactly as written\n",
    "- Rate confidence based on information clarity and completeness\n",
    "- Include partial information with appropriate confidence scoring\n",
    "\n",
    "ðŸ“‹ **OUTPUT FORMAT**:\n",
    "{{\n",
    "  \"companies\": [\n",
    "    {{\n",
    "      \"company_name\": \"Exact Company Name from Text\",\n",
    "      \"founding_date\": \"date exactly as mentioned\",\n",
    "      \"founders\": [\"Founder Name 1\", \"Founder Name 2\"],\n",
    "      \"confidence_score\": 0.85,\n",
    "      \"founding_location\": \"City, Region (if mentioned)\",\n",
    "      \"industry_context\": \"Industry description based on context\"\n",
    "    }}\n",
    "  ]\n",
    "}}\n",
    "\n",
    "ðŸ“„ **SOURCE TEXT**:\n",
    "{paragraph}\n",
    "\n",
    "ðŸ¤– **INTELLIGENCE REPORT**:\n",
    "\"\"\")\n",
    "\n",
    "        # advanced processing chain\n",
    "        self.extraction_chain = (\n",
    "            self.extraction_prompt\n",
    "            | self.llm\n",
    "            | JsonOutputParser()\n",
    "            | RunnableLambda(self._process_with_analytics)\n",
    "        )\n",
    "\n",
    "    def _process_with_analytics(self, raw_result: Dict) -> List[CompanyInfo]:\n",
    "        \"\"\"Process results with advanced analytics and quality metrics.\"\"\"\n",
    "        import time\n",
    "        start_time = time.time()\n",
    "\n",
    "        companies = process_extraction_result_advanced(raw_result, \"\")\n",
    "\n",
    "        processing_time = time.time() - start_time\n",
    "\n",
    "        # Calculating metrics for this batch\n",
    "        if companies:\n",
    "            avg_confidence = sum(c.confidence_score for c in companies) / len(companies)\n",
    "            high_confidence_count = sum(1 for c in companies if c.confidence_score > 0.8)\n",
    "        else:\n",
    "            avg_confidence = 0.0\n",
    "            high_confidence_count = 0\n",
    "\n",
    "        self.processing_metrics.append({\n",
    "            'processing_time': processing_time,\n",
    "            'companies_count': len(companies),\n",
    "            'avg_confidence': avg_confidence,\n",
    "            'high_confidence_count': high_confidence_count\n",
    "        })\n",
    "\n",
    "        return companies\n",
    "\n",
    "    def process_paragraph_advanced(self, paragraph: str, paragraph_index: int = 0) -> List[CompanyInfo]:\n",
    "        \"\"\"Process single paragraph with enhanced error handling and logging.\"\"\"\n",
    "        try:\n",
    "            print(f\"ðŸ”„ Processing paragraph {paragraph_index + 1}... \", end=\"\")\n",
    "\n",
    "            companies = self.extraction_chain.invoke({\"paragraph\": paragraph})\n",
    "\n",
    "            if companies:\n",
    "                print(f\"âœ… Found {len(companies)} companies\")\n",
    "                for company in companies:\n",
    "                    print(f\"   ðŸ¢ {company.company_name} (confidence: {company.confidence_score:.1%})\")\n",
    "            else:\n",
    "                print(\"âŒ No companies found\")\n",
    "\n",
    "            return companies\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"ðŸ’¥ Error: {str(e)}\")\n",
    "            return []\n",
    "\n",
    "    def process_text_with_analytics(self, text: str) -> tuple[List[CompanyInfo], ExtractionMetrics]:\n",
    "        \"\"\"Process complete text with comprehensive analytics generation.\"\"\"\n",
    "        import time\n",
    "        total_start_time = time.time()\n",
    "\n",
    "        paragraphs = split_into_paragraphs(text)\n",
    "        all_companies = []\n",
    "\n",
    "        print(f\"ðŸš€ Starting Advanced Corporate Intelligence Extraction\")\n",
    "        print(f\"ðŸ“„ Processing {len(paragraphs)} text segments...\")\n",
    "        print(\"=\" * 70)\n",
    "\n",
    "        for i, paragraph in enumerate(paragraphs):\n",
    "            companies = self.process_paragraph_advanced(paragraph, i)\n",
    "            all_companies.extend(companies)\n",
    "\n",
    "            # Add delay for rate limiting\n",
    "            time.sleep(0.3)\n",
    "\n",
    "        total_processing_time = time.time() - total_start_time\n",
    "\n",
    "        metrics = self._generate_metrics(all_companies, len(paragraphs), total_processing_time)\n",
    "\n",
    "        print(\"\\n\" + \"=\" * 70)\n",
    "        print(f\"ðŸŽ¯ Extraction Complete! Summary:\")\n",
    "        print(f\"   ðŸ“Š Total Companies: {metrics.total_companies}\")\n",
    "        print(f\"   â­ Average Confidence: {metrics.avg_confidence_score:.1%}\")\n",
    "        print(f\"   ðŸ† High-Confidence Extractions: {metrics.high_confidence_count}\")\n",
    "        print(f\"   â±ï¸  Total Processing Time: {metrics.processing_time_total:.1f}s\")\n",
    "        print(f\"   ðŸ“ˆ Extraction Rate: {metrics.extraction_rate:.2f} companies/paragraph\")\n",
    "\n",
    "        return all_companies, metrics\n",
    "\n",
    "    def _generate_metrics(self, companies: List[CompanyInfo], paragraphs_count: int, total_time: float) -> ExtractionMetrics:\n",
    "        \"\"\"Generate comprehensive quality and performance metrics.\"\"\"\n",
    "        if not companies:\n",
    "            return ExtractionMetrics(\n",
    "                total_companies=0,\n",
    "                avg_confidence_score=0.0,\n",
    "                high_confidence_count=0,\n",
    "                processing_time_total=total_time,\n",
    "                paragraphs_processed=paragraphs_count,\n",
    "                extraction_rate=0.0\n",
    "            )\n",
    "\n",
    "        avg_confidence = sum(c.confidence_score for c in companies) / len(companies)\n",
    "        high_confidence_count = sum(1 for c in companies if c.confidence_score > 0.8)\n",
    "        extraction_rate = len(companies) / max(paragraphs_count, 1)\n",
    "\n",
    "        return ExtractionMetrics(\n",
    "            total_companies=len(companies),\n",
    "            avg_confidence_score=avg_confidence,\n",
    "            high_confidence_count=high_confidence_count,\n",
    "            processing_time_total=total_time,\n",
    "            paragraphs_processed=paragraphs_count,\n",
    "            extraction_rate=extraction_rate\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "zdzmocUIZR1w",
    "outputId": "17f93177-dabc-4897-bbc4-8088415fe2b2"
   },
   "outputs": [],
   "source": [
    "print(\"ðŸš€ Initializing Advanced Corporate Intelligence Extraction Pipeline...\")\n",
    "print(\"ðŸ”§ Loading AI models and configuring enhanced analytics...\")\n",
    "print(\"âœ… Pipeline ready with advanced features enabled!\")\n",
    "\n",
    "advanced_pipeline = AdvancedCompanyExtractionPipeline(llm)\n",
    "\n",
    "print(\"\\nðŸŽ¯ Pipeline Features:\")\n",
    "print(\"   â€¢ Multi-modal data extraction\")\n",
    "print(\"   â€¢ Confidence scoring and validation\")\n",
    "print(\"   â€¢ Industry classification\")\n",
    "print(\"   â€¢ Geographic intelligence\")\n",
    "print(\"   â€¢ Real-time quality metrics\")\n",
    "print(\"   â€¢ Advanced analytics generation\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "t7VCa-53Zegm"
   },
   "source": [
    "### Processing All Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ZlwDb0ndZTzy",
    "outputId": "43df2940-506f-480b-e751-e4ba333a0eaf"
   },
   "outputs": [],
   "source": [
    "# Advanced processing with comprehensive analytics\n",
    "print(\"ðŸš€ Initiating Advanced Corporate Intelligence Extraction...\")\n",
    "print(\"ðŸ” Applying AI-powered analysis with confidence scoring...\")\n",
    "\n",
    "all_companies, extraction_metrics = advanced_pipeline.process_text_with_analytics(essay_text)\n",
    "\n",
    "print(f\"\\nðŸŽ‰ Advanced Extraction Results:\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Enhanced results display with categorization\n",
    "high_confidence_companies = [c for c in all_companies if c.confidence_score > 0.8]\n",
    "medium_confidence_companies = [c for c in all_companies if 0.5 < c.confidence_score <= 0.8]\n",
    "low_confidence_companies = [c for c in all_companies if c.confidence_score <= 0.5]\n",
    "\n",
    "print(f\"\\nðŸ† HIGH CONFIDENCE EXTRACTIONS ({len(high_confidence_companies)}):\")\n",
    "for i, company in enumerate(high_confidence_companies, 1):\n",
    "    print(f\"{i:2d}. ðŸ¢ {company.company_name}\")\n",
    "    print(f\"    ðŸ“… Founded: {company.founding_date}\" + (f\" ({company.company_age} years)\" if company.company_age else \"\"))\n",
    "    print(f\"    ðŸ‘¥ Founders: {', '.join(company.founders)}\")\n",
    "    if company.founding_location:\n",
    "        print(f\"    ðŸ“ Location: {company.founding_location}\")\n",
    "    if company.industry_category:\n",
    "        print(f\"    ðŸ­ Industry: {company.industry_category}\")\n",
    "    print(f\"    â­ Confidence: {company.confidence_score:.1%}\")\n",
    "    print()\n",
    "\n",
    "if medium_confidence_companies:\n",
    "    print(f\"\\nâš ï¸  MEDIUM CONFIDENCE EXTRACTIONS ({len(medium_confidence_companies)}):\")\n",
    "    for i, company in enumerate(medium_confidence_companies, 1):\n",
    "        print(f\"{i:2d}. ðŸ¢ {company.company_name} - {company.founding_date} (â­ {company.confidence_score:.1%})\")\n",
    "\n",
    "if low_confidence_companies:\n",
    "    print(f\"\\nðŸ” NEEDS REVIEW ({len(low_confidence_companies)}):\")\n",
    "    for i, company in enumerate(low_confidence_companies, 1):\n",
    "        print(f\"{i:2d}. ðŸ¢ {company.company_name} - {company.founding_date} (â­ {company.confidence_score:.1%})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hN_LN8dvaNtv"
   },
   "source": [
    "### Saving to CSV File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "uV0340YAaOeL",
    "outputId": "392ef447-0414-4ccb-c3b7-19572deb9373"
   },
   "outputs": [],
   "source": [
    "def export_enhanced_results(companies: List[CompanyInfo], metrics: ExtractionMetrics,\n",
    "                           analytics: IndustryAnalytics, base_filename: str = \"company_info\"):\n",
    "    \"\"\"Advanced multi-format export system with comprehensive reporting.\"\"\"\n",
    "\n",
    "    # 1. PRIMARY CSV EXPORT (Standard Format)\n",
    "    print(\"ðŸ“ Exporting Primary Results...\")\n",
    "    primary_data = []\n",
    "    for i, company in enumerate(companies, 1):\n",
    "        primary_data.append({\n",
    "            \"S.N.\": i,\n",
    "            \"Company Name\": company.company_name,\n",
    "            \"Founded in\": company.founding_date,\n",
    "            \"Founded by\": str(company.founders).replace(\"[\", \"\").replace(\"]\", \"\").replace(\"'\", \"\")\n",
    "        })\n",
    "\n",
    "    primary_df = pd.DataFrame(primary_data)\n",
    "    primary_filename = f\"{base_filename}.csv\"\n",
    "    primary_df.to_csv(primary_filename, index=False)\n",
    "    print(f\"âœ… Primary CSV exported: {primary_filename}\")\n",
    "\n",
    "    # 2. ENHANCED CSV EXPORT (With All Fields)\n",
    "    enhanced_data = []\n",
    "    for i, company in enumerate(companies, 1):\n",
    "        enhanced_data.append({\n",
    "            \"S.N.\": i,\n",
    "            \"Company Name\": company.company_name,\n",
    "            \"Founded in\": company.founding_date,\n",
    "            \"Founded by\": \", \".join(company.founders),\n",
    "            \"Confidence Score\": f\"{company.confidence_score:.3f}\",\n",
    "            \"Company Age\": company.company_age or \"Unknown\",\n",
    "            \"Founder Count\": company.founder_count,\n",
    "            \"Location\": company.founding_location or \"Not specified\",\n",
    "            \"Industry\": company.industry_category or \"Not classified\"\n",
    "        })\n",
    "\n",
    "    enhanced_df = pd.DataFrame(enhanced_data)\n",
    "    enhanced_filename = f\"{base_filename}_enhanced.csv\"\n",
    "    enhanced_df.to_csv(enhanced_filename, index=False)\n",
    "    print(f\"âœ… Enhanced CSV exported: {enhanced_filename}\")\n",
    "\n",
    "    # 3. QUALITY METRICS REPORT\n",
    "    metrics_data = {\n",
    "        \"Metric\": [\n",
    "            \"Total Companies Extracted\",\n",
    "            \"Average Confidence Score\",\n",
    "            \"High-Confidence Extractions\",\n",
    "            \"Processing Time (seconds)\",\n",
    "            \"Paragraphs Processed\",\n",
    "            \"Extraction Rate (companies/paragraph)\"\n",
    "        ],\n",
    "        \"Value\": [\n",
    "            metrics.total_companies,\n",
    "            f\"{metrics.avg_confidence_score:.1%}\",\n",
    "            f\"{metrics.high_confidence_count} ({metrics.high_confidence_count/metrics.total_companies:.1%})\",\n",
    "            f\"{metrics.processing_time_total:.2f}\",\n",
    "            metrics.paragraphs_processed,\n",
    "            f\"{metrics.extraction_rate:.2f}\"\n",
    "        ]\n",
    "    }\n",
    "\n",
    "    metrics_df = pd.DataFrame(metrics_data)\n",
    "    metrics_filename = f\"{base_filename}_quality_report.csv\"\n",
    "    metrics_df.to_csv(metrics_filename, index=False)\n",
    "    print(f\"âœ… Quality Report exported: {metrics_filename}\")\n",
    "\n",
    "    # 4. ANALYTICS SUMMARY\n",
    "    analytics_data = []\n",
    "\n",
    "    # Industry breakdown\n",
    "    for industry, count in analytics.industry_distribution.items():\n",
    "        analytics_data.append({\n",
    "            \"Category\": \"Industry\",\n",
    "            \"Subcategory\": industry,\n",
    "            \"Count\": count,\n",
    "            \"Percentage\": f\"{(count/metrics.total_companies)*100:.1f}%\"\n",
    "        })\n",
    "\n",
    "    # Decade breakdown\n",
    "    for decade, count in analytics.decade_distribution.items():\n",
    "        analytics_data.append({\n",
    "            \"Category\": \"Founding Decade\",\n",
    "            \"Subcategory\": decade,\n",
    "            \"Count\": count,\n",
    "            \"Percentage\": f\"{(count/metrics.total_companies)*100:.1f}%\"\n",
    "        })\n",
    "\n",
    "    analytics_df = pd.DataFrame(analytics_data)\n",
    "    analytics_filename = f\"{base_filename}_analytics.csv\"\n",
    "    analytics_df.to_csv(analytics_filename, index=False)\n",
    "    print(f\"âœ… Analytics Report exported: {analytics_filename}\")\n",
    "\n",
    "    # Display primary results preview\n",
    "    print(f\"\\nðŸ“‹ PRIMARY CSV PREVIEW ({primary_filename}):\")\n",
    "    print(\"=\" * 80)\n",
    "    print(primary_df.head(10).to_string(index=False))\n",
    "\n",
    "    return {\n",
    "        \"primary_df\": primary_df,\n",
    "        \"enhanced_df\": enhanced_df,\n",
    "        \"metrics_df\": metrics_df,\n",
    "        \"analytics_df\": analytics_df,\n",
    "        \"files_created\": [primary_filename, enhanced_filename, metrics_filename, analytics_filename]\n",
    "    }\n",
    "\n",
    "# Execute enhanced export\n",
    "print(\"ðŸš€ Initiating Enhanced Multi-Format Export System...\")\n",
    "export_results = export_enhanced_results(all_companies, extraction_metrics, analytics)\n",
    "\n",
    "print(f\"\\nðŸŽ‰ Export Complete! Created {len(export_results['files_created'])} files:\")\n",
    "for filename in export_results['files_created']:\n",
    "    print(f\"   ðŸ“„ {filename}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "REOx-WVpadCP",
    "outputId": "b0726f0d-3a78-4f6d-f8ae-8db182056151"
   },
   "outputs": [],
   "source": [
    "# ðŸŽ¯ FINAL COMPREHENSIVE RESULTS DASHBOARD\n",
    "print(\"=\" * 80)\n",
    "print(\"ðŸ† CORPORATE INTELLIGENCE EXTRACTION - FINAL REPORT\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "primary_df = export_results['primary_df']\n",
    "enhanced_df = export_results['enhanced_df']\n",
    "\n",
    "print(f\"\\nðŸ“Š EXTRACTION SUMMARY:\")\n",
    "print(f\"   ðŸ¢ Total Companies Identified: {len(all_companies)}\")\n",
    "print(f\"   â­ Average Confidence Score: {extraction_metrics.avg_confidence_score:.1%}\")\n",
    "print(f\"   ðŸ† High-Confidence Extractions: {extraction_metrics.high_confidence_count}\")\n",
    "print(f\"   â±ï¸  Processing Time: {extraction_metrics.processing_time_total:.1f} seconds\")\n",
    "print(f\"   ðŸ“ˆ Extraction Efficiency: {extraction_metrics.extraction_rate:.2f} companies/paragraph\")\n",
    "\n",
    "print(f\"\\nðŸŽ¯ QUALITY BREAKDOWN:\")\n",
    "high_conf = len([c for c in all_companies if c.confidence_score > 0.8])\n",
    "med_conf = len([c for c in all_companies if 0.5 < c.confidence_score <= 0.8])\n",
    "low_conf = len([c for c in all_companies if c.confidence_score <= 0.5])\n",
    "\n",
    "print(f\"   ðŸŸ¢ High Confidence (>80%): {high_conf} companies ({high_conf/len(all_companies)*100:.1f}%)\")\n",
    "print(f\"   ðŸŸ¡ Medium Confidence (50-80%): {med_conf} companies ({med_conf/len(all_companies)*100:.1f}%)\")\n",
    "print(f\"   ðŸ”´ Low Confidence (<50%): {low_conf} companies ({low_conf/len(all_companies)*100:.1f}%)\")\n",
    "\n",
    "print(f\"\\nðŸ“‹ STANDARD CSV FORMAT (company_info.csv):\")\n",
    "print(\"-\" * 60)\n",
    "print(primary_df.to_string(index=False, max_rows=15))\n",
    "\n",
    "if len(primary_df) > 15:\n",
    "    print(f\"\\n... and {len(primary_df) - 15} more companies\")\n",
    "\n",
    "print(f\"\\nðŸŽ‰ SUCCESS! Advanced Corporate Intelligence Extraction Complete!\")\n",
    "print(f\"ðŸ“ All results exported to multiple formats for comprehensive analysis.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-BDk_84-9ekS"
   },
   "source": [
    "## ðŸ“Š Advanced Analytics & Insights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "DXiEFSKr9ekS",
    "outputId": "8c21b0c6-5c29-4a20-d7e7-d3c6b50bbce7"
   },
   "outputs": [],
   "source": [
    "def generate_industry_analytics(companies: List[CompanyInfo]) -> IndustryAnalytics:\n",
    "    \"\"\"Generate comprehensive industry and business analytics.\"\"\"\n",
    "\n",
    "    # Industry distribution\n",
    "    industry_counts = {}\n",
    "    for company in companies:\n",
    "        industry = company.industry_category or \"Unknown\"\n",
    "        industry_counts[industry] = industry_counts.get(industry, 0) + 1\n",
    "\n",
    "    # Decade distribution\n",
    "    decade_counts = {}\n",
    "    for company in companies:\n",
    "        try:\n",
    "            year = int(company.founding_date.split('-')[0])\n",
    "            decade = f\"{year//10*10}s\"\n",
    "            decade_counts[decade] = decade_counts.get(decade, 0) + 1\n",
    "        except:\n",
    "            decade_counts[\"Unknown\"] = decade_counts.get(\"Unknown\", 0) + 1\n",
    "\n",
    "    # Founder analytics\n",
    "    all_founders = []\n",
    "    founder_counts = {}\n",
    "    for company in companies:\n",
    "        all_founders.extend(company.founders)\n",
    "        count = company.founder_count\n",
    "        founder_counts[count] = founder_counts.get(count, 0) + 1\n",
    "\n",
    "    founder_analytics = {\n",
    "        \"total_unique_founders\": len(set(all_founders)),\n",
    "        \"avg_founders_per_company\": sum(c.founder_count for c in companies) / len(companies) if companies else 0,\n",
    "        \"founder_count_distribution\": founder_counts,\n",
    "        \"most_common_founder_names\": {},  \n",
    "    }\n",
    "\n",
    "    # Geographic distribution\n",
    "    geo_counts = {}\n",
    "    for company in companies:\n",
    "        location = company.founding_location or \"Unknown\"\n",
    "        geo_counts[location] = geo_counts.get(location, 0) + 1\n",
    "\n",
    "    return IndustryAnalytics(\n",
    "        industry_distribution=industry_counts,\n",
    "        decade_distribution=decade_counts,\n",
    "        founder_analytics=founder_analytics,\n",
    "        geographic_distribution=geo_counts\n",
    "    )\n",
    "\n",
    "print(\"ðŸ“Š Generating Advanced Business Intelligence...\")\n",
    "analytics = generate_industry_analytics(all_companies)\n",
    "\n",
    "print(\"\\nðŸ­ INDUSTRY ANALYSIS:\")\n",
    "print(\"-\" * 40)\n",
    "for industry, count in sorted(analytics.industry_distribution.items(), key=lambda x: x[1], reverse=True):\n",
    "    percentage = (count / len(all_companies)) * 100\n",
    "    print(f\"{industry:<20} {count:>3} companies ({percentage:>5.1f}%)\")\n",
    "\n",
    "print(\"\\nðŸ“… FOUNDING DECADE ANALYSIS:\")\n",
    "print(\"-\" * 40)\n",
    "for decade, count in sorted(analytics.decade_distribution.items()):\n",
    "    if decade != \"Unknown\":\n",
    "        percentage = (count / len(all_companies)) * 100\n",
    "        print(f\"{decade:<10} {count:>3} companies ({percentage:>5.1f}%)\")\n",
    "\n",
    "print(\"\\nðŸ‘¥ FOUNDER INSIGHTS:\")\n",
    "print(\"-\" * 30)\n",
    "founder_stats = analytics.founder_analytics\n",
    "print(f\"Total Unique Founders: {founder_stats['total_unique_founders']}\")\n",
    "print(f\"Average Founders per Company: {founder_stats['avg_founders_per_company']:.1f}\")\n",
    "print(\"\\nFounder Count Distribution:\")\n",
    "for count, companies_count in sorted(founder_stats['founder_count_distribution'].items()):\n",
    "    print(f\"  {count} founder(s): {companies_count} companies\")\n",
    "\n",
    "print(\"\\nðŸŒ GEOGRAPHIC DISTRIBUTION:\")\n",
    "print(\"-\" * 35)\n",
    "top_locations = sorted(analytics.geographic_distribution.items(),\n",
    "                      key=lambda x: x[1], reverse=True)[:10]\n",
    "for location, count in top_locations:\n",
    "    if location != \"Unknown\":\n",
    "        percentage = (count / len(all_companies)) * 100\n",
    "        print(f\"{location:<25} {count:>2} companies ({percentage:>4.1f}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aqFkGGto9ekS"
   },
   "source": [
    "### ðŸŽ¯ System Performance & Capabilities Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0sbScF2Q9ekS",
    "outputId": "b52b6e7e-5899-4821-b92c-da7b7b2daef7"
   },
   "outputs": [],
   "source": [
    "print(\"ðŸš€ ADVANCED CORPORATE INTELLIGENCE SYSTEM - CAPABILITIES SHOWCASE\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "print(\"\\nðŸ“Š SYSTEM PERFORMANCE METRICS:\")\n",
    "print(f\"   Processing Speed: {extraction_metrics.processing_time_total:.1f} seconds total\")\n",
    "print(f\"   Accuracy Rate: {extraction_metrics.avg_confidence_score:.1%} average confidence\")\n",
    "print(f\"   Quality Distribution: {extraction_metrics.high_confidence_count}/{extraction_metrics.total_companies} high-confidence extractions\")\n",
    "print(f\"   Coverage Rate: {extraction_metrics.extraction_rate:.2f} companies per paragraph\")"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
